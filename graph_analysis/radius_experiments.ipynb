{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "dclient = Client(memory_limit='60GB', n_workers=8)\n",
    "\n",
    "import dask\n",
    "from dask import delayed\n",
    "from dask.delayed import delayed\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import dask.bag as db\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "import scipy.sparse.linalg\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "import subprocess\n",
    "from os.path import exists\n",
    "\n",
    "# Stolen from WH example for convenience. Not all used.\n",
    "\n",
    "os.chdir(\"/home/ec2-user/Contacts-sensitive/\")\n",
    "session = boto3.session.Session(profile_name='yse-bioecon')\n",
    "client = boto3.client('s3')\n",
    "\n",
    "\n",
    "\n",
    "os.chdir(\"/home/ec2-user/Contacts-sensitive/functions\")\n",
    "dclient.upload_file('aws_um_funcs.py')\n",
    "import aws_um_funcs as fcombo\n",
    "\n",
    "os.chdir(\"/home/ec2-user/Contacts-sensitive/clump_filtering\")\n",
    "dclient.upload_file('clump_finding.py')\n",
    "import clump_finding\n",
    "\n",
    "#os.system(\"export PYTHONPATH=~/Contacts-sensitive/:~/Contacts-sensitive/functions/\")\n",
    "os.chdir(\"/home/ec2-user/Contacts-sensitive/\")\n",
    "\n",
    "sql_exp = (\"SELECT * \" +\n",
    "           \"FROM s3Object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/ec2-user/Contacts-sensitive/functions\")\n",
    "dclient.upload_file('aws_um_funcs.py')\n",
    "import aws_um_funcs as fcombo\n",
    "\n",
    "os.chdir(\"/home/ec2-user/Contacts-sensitive/clump_filtering\")\n",
    "dclient.upload_file('clump_finding.py')\n",
    "import clump_finding\n",
    "\n",
    "import time\n",
    "import networkx as nx\n",
    "from copy import deepcopy\n",
    "\n",
    "os.chdir(\"/home/ec2-user/Contacts-sensitive/graph_analysis\")\n",
    "import timestep_analysis\n",
    "\n",
    "def contacts_to_matrix(df, weighting='per_contact', return_only_largest_component=True, trim_weights_to=None, matrix='laplacian'):\n",
    "    \"\"\"\n",
    "    Generates the Laplacian or Adjacency matrix of the contact graph over the day. Can be modified for edge weighting.\n",
    "    \n",
    "    If return_weights is passed, instead just returns the edge weights.\n",
    "    \"\"\"\n",
    "    if 'weight' in df.columns:\n",
    "        weights = df\n",
    "    else:\n",
    "        weights = timestep_analysis.sum_contacts_by_pairs([df], weighting=weighting)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(weights[['device_id_1', 'device_id_2', 'weight']].values)\n",
    "    #print(max(dict(G.edges).items(), key=lambda x: x[1]['weight']))\n",
    "    print(sorted([len(i) for i in nx.connected_components(G)])[-10:])\n",
    "    if return_only_largest_component:\n",
    "        G = G.subgraph(max(nx.connected_components(G), key=len))\n",
    "    A = nx.to_scipy_sparse_matrix(G, dtype=np.float64, format='csc')\n",
    "    if matrix == 'adjacency':\n",
    "        return A\n",
    "    elif matrix == 'laplacian':\n",
    "        if trim_weights_to is not None:\n",
    "            A.data = np.minimum(A.data, trim_weights_to)\n",
    "\n",
    "        degs = np.squeeze(np.asarray(A.sum(axis=0)))\n",
    "        D = scipy.sparse.diags(degs, format='csc')\n",
    "        L = D - A\n",
    "        return L\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loading import date_iterator, load_date_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask.distributed import Client\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "os.chdir(\"/home/ec2-user/Contacts-sensitive/graph_analysis\")\n",
    "import data_loading, timestep_analysis\n",
    "from sksparse.cholmod import cholesky\n",
    "\n",
    "def per_device_exposure_index(contact_df, case_frequencies):\n",
    "    \"\"\"\n",
    "    Calculates, for each device, an exposure index based on how much contact they had and where it was (i.e. case exposure)\n",
    "    \"\"\"\n",
    "    if not isinstance(case_frequencies, dict):\n",
    "        case_frequencies = case_frequencies.to_dict()\n",
    "    exposures = data_loading.get_cty_row(contact_df).map(case_frequencies)\n",
    "    id_1_sum = pd.DataFrame()\n",
    "    id_2_sum = pd.DataFrame()\n",
    "    id_1_sum['did'] = contact_df['device_id_1'].values\n",
    "    id_2_sum['did'] = contact_df['device_id_2'].values\n",
    "    id_1_sum['exposures'] = exposures.values\n",
    "    id_2_sum['exposures'] = exposures.values\n",
    "    id_1_sum = id_1_sum.groupby('did').sum()\n",
    "    id_2_sum = id_2_sum.groupby('did').sum()\n",
    "    \n",
    "    return id_1_sum.add(id_2_sum, fill_value=0)\n",
    "\n",
    "case_frequencies = data_loading.rename(data_loading.generator_function_arg_setter(data_loading.cty_case_rate_generator, {'time_average_period': 7}), 'case_frequencies')\n",
    "\n",
    "def random_bits_from_distribution(distribution):\n",
    "    # Given a vector of probabilities, returns a vector of bits of the same shape, where the bit at index I is 1 with probability distribution[I]\n",
    "    return (np.random.uniform(size=distribution.shape) < distribution).astype(np.int)\n",
    "\n",
    "def weights_df_to_laplacian(df, index=None):  # Given a df with at most one entry per contact, makes the laplacian with indexing by index (the ordered list of ids)\n",
    "    if index is None:\n",
    "        index = list(set(df['device_id_1'].tolist() + df['device_id_2'].tolist()))\n",
    "    index_map = {ind: i for i, ind in enumerate(index)}\n",
    "    li = len(index_map)\n",
    "    L = scipy.sparse.dok_matrix((li, li))\n",
    "    L[df['device_id_1'].map(index_map).to_numpy(), df['device_id_2'].map(index_map).to_numpy()] = -df['weight'].to_numpy()\n",
    "    L[df['device_id_2'].map(index_map).to_numpy(), df['device_id_1'].map(index_map).to_numpy()] = -df['weight'].to_numpy()\n",
    "    L[np.arange(li), np.arange(li)] = -L.sum(axis=0)\n",
    "    return L.tocsr()\n",
    "\n",
    "def weights_df_to_adjacency(df, index=None):  # Given a df with at most one entry per contact, makes the adjacency matrix with indexing by index (the ordered list of ids)\n",
    "    if index is None:\n",
    "        index = list(set(df['device_id_1'].tolist() + df['device_id_2'].tolist()))\n",
    "    index_map = {ind: i for i, ind in enumerate(index)}\n",
    "    li = len(index_map)\n",
    "    A = scipy.sparse.dok_matrix((li, li))\n",
    "    A[df['device_id_1'].map(index_map).to_numpy(), df['device_id_2'].map(index_map).to_numpy()] = df['weight'].to_numpy()\n",
    "    A[df['device_id_2'].map(index_map).to_numpy(), df['device_id_1'].map(index_map).to_numpy()] = df['weight'].to_numpy()\n",
    "    return A.tocsr()\n",
    "    \n",
    "\n",
    "class ConvergenceError(Exception):\n",
    "    pass\n",
    "\n",
    "def k_low_eigs_from_laplacian_cg(L, k, iters=100, eps=1e-5):\n",
    "    n = L.shape[0]\n",
    "    L = L + scipy.sparse.eye(n, format='csr') * eps\n",
    "    current_eigvecs = [np.ones(shape=(n,)) / np.sqrt(n)]\n",
    "    current_eigs = [eps]\n",
    "    for _ in range(k):\n",
    "        evec = np.random.normal(size=(n,))\n",
    "        titers = 0\n",
    "        while titers < 10000:\n",
    "            for ce in current_eigvecs:\n",
    "                evec -= ce * np.inner(ce, evec)\n",
    "            evec /= np.linalg.norm(evec)\n",
    "            grad = (L@evec[:, np.newaxis])\n",
    "            for ce in current_eigvecs:\n",
    "                grad -= ce[:, np.newaxis] * (ce[np.newaxis] @ grad)\n",
    "            \n",
    "            Lg = L @ grad\n",
    "            c = (evec[np.newaxis] @ Lg)[0][0] / (grad.T @ Lg)[0][0]\n",
    "            \n",
    "            # (e-cg)A(e-cg) (e-cg)Ag=0\n",
    "            # eAg=cgAg c = eAg/gAg\n",
    "            nevec = evec - c * grad[:, 0]\n",
    "            nevec /= np.linalg.norm(nevec)\n",
    "            error = np.linalg.norm(nevec - evec)\n",
    "            old_eig_est = np.sqrt((evec[np.newaxis] @ (L@evec[:, np.newaxis]))[0][0])\n",
    "            new_eig_est = np.sqrt((nevec[np.newaxis] @ L @ nevec[:, np.newaxis])[0][0])\n",
    "            eig_est_diff = abs(new_eig_est - old_eig_est)\n",
    "            evec = nevec\n",
    "            if error < .0001:\n",
    "                break\n",
    "            else:\n",
    "                if titers % 100 == 0:\n",
    "                    print(np.linalg.norm(evec), old_eig_est, np.linalg.norm(nevec), new_eig_est)\n",
    "                    print(\"Finished iteration\", titers, \"with error:\", error, \"eig error:\", eig_est_diff, \"eig estimate:\", new_eig_est)\n",
    "                titers += 1\n",
    "        current_eigvecs += [evec]\n",
    "        current_eigs += [new_eig_est]\n",
    "    return [x - eps for x in current_eigs[1:]]\n",
    "\n",
    "def k_low_eigs_from_laplacian(L, k, iters=100, eps=1e-5):\n",
    "    n = L.shape[0]\n",
    "    L = L + scipy.sparse.eye(n, format='csr') * eps\n",
    "    current_eigvecs = [np.ones(shape=(n,)) / np.sqrt(n)]\n",
    "    current_eigs = [eps]\n",
    "    for _ in range(k):\n",
    "        evec = np.random.normal(size=(n,))\n",
    "        for __ in range(iters):\n",
    "            for ce in current_eigvecs:\n",
    "                evec -= ce * np.inner(ce, evec)\n",
    "            evec /= np.linalg.norm(evec)\n",
    "            evec, conv = scipy.sparse.linalg.cg(L, evec)\n",
    "            print(\"Iter!\")\n",
    "            if conv != 0:\n",
    "                raise CustomError(\"Convergence failed with return value \" + str(conv))\n",
    "        current_eigvecs += [evec / np.linalg.norm(evec)]\n",
    "        current_eigs += [1 / np.linalg.norm(evec)]\n",
    "    return [x - eps for x in current_eigs[1:]]\n",
    "\n",
    "def k_low_eigs_from_laplacian_cholesky(L, k, iters=100, eps=1e-4):  # Not delayable\n",
    "    n = L.shape[0]\n",
    "    chol = cholesky((L + scipy.sparse.eye(n, format='csr') * eps).tocsc())\n",
    "    current_eigvecs = [np.ones(shape=(n,)) / np.sqrt(n)]\n",
    "    current_eigs = [eps]\n",
    "    for _ in range(k):\n",
    "        evec = np.random.normal(size=(n,))\n",
    "        for __ in range(iters):\n",
    "            for ce in current_eigvecs:\n",
    "                evec -= ce * np.inner(ce, evec)\n",
    "            evec /= np.linalg.norm(evec)\n",
    "            evec = chol.solve_A(evec)\n",
    "        current_eigvecs += [evec / np.linalg.norm(evec)]\n",
    "        current_eigs += [1 / np.linalg.norm(evec)]\n",
    "    return [x - eps for x in current_eigs[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigs_from_sum_contacts_by_pairs(indata, k=6, iters=100):  # NOTE: NOT USING CHOLESKY ANYMORE\n",
    "    if indata[0][-1] is None:\n",
    "        return None\n",
    "    else:\n",
    "        L = contacts_to_matrix(pd.concat(indata[0]))\n",
    "        return k_low_eigs_from_laplacian(L, k, iters=iters)   # ============NOTE WHICH OF ABOVE WE ARE USING\n",
    "    \n",
    "def adjacency_eigs_from_sum_contacts_by_pairs_uniform_weight(indata, k=6):\n",
    "    if indata[0][-1] is None:\n",
    "        return None\n",
    "    else:\n",
    "        A = contacts_to_matrix(pd.concat(indata[0]), matrix='adjacency', trim_weights_to=1)\n",
    "        return np.sort(scipy.sparse.linalg.eigsh(A, which='LA', k=k)[0])[::-1]\n",
    "\n",
    "def adjacency_eigs_from_sum_contacts_by_pairs_nonuniform_weight(indata, k=6):\n",
    "    if indata[0][-1] is None:\n",
    "        return None\n",
    "    else:\n",
    "        A = contacts_to_matrix(pd.concat(indata[0]), matrix='adjacency')\n",
    "        return np.sort(scipy.sparse.linalg.eigsh(A, which='LA', k=k)[0])[::-1]\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "The below uses Dan Spielman's laplacians.jl repo to speed up computation dramatically. We use PyCall (on the Julia end) and PyJulia (on the Python end) to interface\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from julia.api import Julia\n",
    "import os\n",
    "import random\n",
    "\n",
    "def eigs_from_sum_contacts_by_pairs_laplaciansjl(indata, k=6, weighting='per_contact'):  # Laplacians.fiedler is kind enough to just pull lowest nonzero eigs, so we don't have to worry about splitting connected components here.\n",
    "    if indata[0][-1] is None:\n",
    "        return None\n",
    "    else:\n",
    "        all_data = pd.concat(indata[0])\n",
    "        weights = timestep_analysis.sum_contacts_by_pairs(all_data, weighting=weighting)\n",
    "        index = set(weights['device_id_1'].tolist() + weights['device_id_2'].tolist())\n",
    "        index_map = {ind: i+1 for i, ind in enumerate(index)}  # laplacians.jl seems to want indices to begin with 1.\n",
    "        weights['device_id_1'] = [index_map[x] for x in weights['device_id_1']]\n",
    "        weights['device_id_2'] = [index_map[x] for x in weights['device_id_2']]\n",
    "        array_string = weights.to_csv(header=False, index=False)\n",
    "        \n",
    "        tempfilename = str(random.randrange(10000000000000000))+'.tmp'\n",
    "        with open(tempfilename, \"a\") as f:\n",
    "            f.write(array_string)\n",
    "        \n",
    "        import julia\n",
    "        jl = Julia(runtime='/home/ec2-user/julia-1.7.2/bin/julia', compiled_modules=False)\n",
    "        from julia import Main\n",
    "        Main.include(\"/home/ec2-user/Laplacians.jl\")\n",
    "        Main.in_string = array_string\n",
    "        data = jl.eval(\"import Laplacians; out_data = Laplacians.fiedler(Laplacians.read_graph(\" + '\"' + tempfilename + '\"' + \"), nev=\" + str(k) + \"); return(out_data)\")\n",
    "        os.remove(tempfilename)\n",
    "        return np.array(data[0])\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456691\n"
     ]
    }
   ],
   "source": [
    "df = load_date_interval('WY', (2020, 1, 3), (2020, 1, 4))\n",
    "print(len(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14, 15, 16, 22, 23, 24, 26, 44, 79, 38055]\n"
     ]
    }
   ],
   "source": [
    "m = contacts_to_matrix(timestep_analysis.sum_contacts_by_pairs(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_contacts = timestep_analysis.sum_contacts_by_pairs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.head(1000000)\n",
    "test_sum = timestep_analysis.sum_contacts_by_pairs(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigs = eigs_from_sum_contacts_by_pairs_laplaciansjl([[test_sum]], k=2)\n",
    "print(eigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No preexisting computed data found!\n",
      "Starting processing loop...\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200105\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200109\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200113\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200117\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200121\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200125\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200129\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200202\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200206\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200210\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200214\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200218\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200222\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200226\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200301\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200305\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200309\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200313\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200317\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200321\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200325\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200329\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200402\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200406\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200410\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200414\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200418\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200422\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200426\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200430\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200504\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200508\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200512\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200516\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200520\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200524\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200528\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200601\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200605\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200609\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200613\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20200617\n",
      "Pre-compute\n"
     ]
    }
   ],
   "source": [
    "timestep_analysis.rolling_day_data('Connecticut',\n",
    "                                   (2020, 1, 1),\n",
    "                                   (2022, 1, 1),\n",
    "                                   external_data_generator_functions=[], \n",
    "                                   per_day_processing_functions=[timestep_analysis.sum_contacts_by_pairs],\n",
    "                                   final_processing_function=eigs_from_sum_contacts_by_pairs_laplaciansjl,\n",
    "                                   fpref='eig_data/eigs_from_sum_contacts_by_pairs_laplaciansjl',\n",
    "                                   num_parallel=4,\n",
    "                                   history_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning state: Alabama\n",
      "Found 365 days of already computed data!\n",
      "Starting processing loop...\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20210101\n",
      "Beginning state: Alaska\n",
      "Found 365 days of already computed data!\n",
      "Starting processing loop...\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20210101\n",
      "Beginning state: American Samoa\n",
      "Found 365 days of already computed data!\n",
      "Starting processing loop...\n",
      "KEYGRAB FAILED FOR 20201231\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20210101\n",
      "Beginning state: Arizona\n",
      "Found 365 days of already computed data!\n",
      "Starting processing loop...\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20210101\n",
      "Beginning state: Arkansas\n",
      "Found 365 days of already computed data!\n",
      "Starting processing loop...\n",
      "Pre-compute\n",
      "Post-compute\n",
      "Finished computes up to 20210101\n",
      "Beginning state: California\n",
      "Found 365 days of already computed data!\n",
      "Starting processing loop...\n",
      "Pre-compute\n"
     ]
    }
   ],
   "source": [
    "from data_loading import state_dict\n",
    "for state in state_dict.keys():\n",
    "    print(\"Beginning state:\", state)\n",
    "    timestep_analysis.rolling_day_data(state,\n",
    "                                   (2020, 1, 1),\n",
    "                                   (2021, 1, 1),\n",
    "                                   external_data_generator_functions=[], \n",
    "                                   per_day_processing_functions=[timestep_analysis.sum_contacts_by_pairs],\n",
    "                                   final_processing_function=eigs_from_sum_contacts_by_pairs_laplaciansjl,\n",
    "                                   fpref='eig_data/eigs_from_sum_contacts_by_pairs_laplaciansjl',\n",
    "                                   num_parallel=4,\n",
    "                                   history_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting summary statistics about the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def summary_statistics_from_sum_contacts_by_pairs(indata):\n",
    "    if indata[0][-1] is None:\n",
    "        return None\n",
    "    all_data = pd.concat(indata[0])\n",
    "    G = nx.Graph()\n",
    "    G.add_weighted_edges_from(all_data[['device_id_1', 'device_id_2', 'weight']].values)\n",
    "    G = G.subgraph(max(nx.connected_components(G), key=len))\n",
    "    left_degrees = all_data.groupby('device_id_1').sum().reset_index()\n",
    "    right_degrees = all_data.groupby('device_id_2').sum().reset_index().rename(columns={'device_id_2':'device_id_1'})\n",
    "    degrees = np.array(pd.concat([left_degrees, right_degrees]).groupby('device_id_1').sum()['weight'])\n",
    "    capped_degrees = np.minimum(degrees, 100)\n",
    "    stats = [\n",
    "        G.number_of_nodes(),\n",
    "        G.number_of_edges(),\n",
    "        #nx.average_shortest_path_length(G),\n",
    "        np.average(degrees),  # First moment (see \"Predicting the SARS-CoV-2 effective reproduction number using bulk contact data from mobile phones\")\n",
    "        np.average(degrees * degrees),  # Second moment (see above)\n",
    "        np.average(capped_degrees),\n",
    "        np.average(capped_degrees * capped_degrees),\n",
    "    ]\n",
    "    return np.array(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_date_interval('CT', (2020, 1, 1), (2020, 1, 5))\n",
    "print(\"DFL:\", len(df.index))\n",
    "sum_contacts = timestep_analysis.sum_contacts_by_pairs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.59793000e+05 3.20305900e+06 3.57453030e+01 1.12582716e+04\n",
      " 2.22524161e+01 1.43927733e+03]\n"
     ]
    }
   ],
   "source": [
    "sstats = summary_statistics_from_sum_contacts_by_pairs([[sum_contacts]])\n",
    "print(sstats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning state: Alabama\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Alaska\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: American Samoa\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Arizona\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Arkansas\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: California\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Colorado\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Connecticut\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Delaware\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: District of Columbia\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Florida\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Georgia\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Guam\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Hawaii\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Idaho\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Illinois\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Indiana\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Iowa\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Kansas\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Kentucky\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Louisiana\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Maine\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Maryland\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Massachusetts\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Michigan\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Minnesota\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Mississippi\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Missouri\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Montana\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Nebraska\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Nevada\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: New Hampshire\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: New Jersey\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: New Mexico\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: New York\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: North Carolina\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: North Dakota\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Northern Mariana Islands\n",
      "Found 366 days of already computed data!\n",
      "Starting processing loop...\n",
      "Beginning state: Ohio\n",
      "Found 200 days of already computed data!\n",
      "Starting processing loop...\n",
      "Pre-compute\n"
     ]
    }
   ],
   "source": [
    "from data_loading import state_dict\n",
    "for state in state_dict.keys():\n",
    "    print(\"Beginning state:\", state)\n",
    "    timestep_analysis.rolling_day_data(state,\n",
    "                                   (2020, 1, 1),\n",
    "                                   (2021, 1, 1),\n",
    "                                   external_data_generator_functions=[], \n",
    "                                   per_day_processing_functions=[timestep_analysis.sum_contacts_by_pairs],\n",
    "                                   final_processing_function=summary_statistics_from_sum_contacts_by_pairs,\n",
    "                                   fpref='summary_data/summary_data',\n",
    "                                   num_parallel=4,\n",
    "                                   history_length=1)\n",
    "timestep_analysis.rolling_day_data('Connecticut',\n",
    "                                   (2020, 1, 1),\n",
    "                                   (2022, 1, 1),\n",
    "                                   external_data_generator_functions=[], \n",
    "                                   per_day_processing_functions=[timestep_analysis.sum_contacts_by_pairs],\n",
    "                                   final_processing_function=eigs_from_sum_contacts_by_pairs_laplaciansjl,\n",
    "                                   fpref='eig_data/eigs_from_sum_contacts_by_pairs_laplaciansjl',\n",
    "                                   num_parallel=4,\n",
    "                                   history_length=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "worker2lx",
   "language": "python",
   "name": "worker2lx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
